# Предсказание продаж видеоигр с помощью двухуровнего ансамбля коррекции ошибок 

[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/gpugobrrr)

![Python](https://img.shields.io/badge/Python-3.11-blue)
![R²](https://img.shields.io/badge/R²-0.8040-green)
![Pipeline](https://img.shields.io/badge/Pipeline-CatBoost+XGBoost-orange)

## Описание проекта

Проект посвящен предсказанию продаж видеоигр с использованием ансамбля моделей машинного обучения. Основная идея заключается в комбинировании CatBoost в качестве базовой модели с XGBoost для коррекции остатков (residual modeling).
Так же проект содержит основные шаги разработки, инсайты и обоснование отказа от bagging ансамблирования в пользу residual modelling. 

**Итоговый результат: R² = 0.8040 (улучшение в +8.63% за счёт ансамбля)**

## Оглавление

1. **Introduction**

2. **Data Validation & Preparing**

3. **Machine Learning Pipeline**
   - Analytics
   - Stratification & Splitting
   - Feature Engineering
   - Building Bagging Ensemble
   - Residual Modeling

4. **Conclusion & Additions**
   - Final Results
   - Work Done
   - Hyperparameter Optimization
   - Insight

## Технический стек

### Основные библиотеки
- **Данные**: `pandas`, `numpy`
- **Машинное обучение**: 
  - `catboost` - градиентный бустинг для основной модели
  - `xgboost` - градиентный бустинг для residual модели
  - `scikit-learn` - Random Forest, Extra Trees, метрики
- **Оптимизация**: `optuna`, `scipy`
- **Визуализация**: `matplotlib`

### Ключевые методы
- **Стратификация**: разбиение данных с сохранением распределения целевого признака
- **Логарифмическое преобразование**: для работы с сильно скошенным распределением продаж
- **Residual modeling**: обучение второй модели на ошибках первой
- **Posterior sampling**: для улучшения качества CatBoost

## Архитектура решения

```
Исходные данные
      ↓
Очистка и валидация
      ↓
Feature Engineering
      ↓
┌─────────────────┐             ┌──────────────────┐
│   CatBoost      │ Предсказания│    Residual      │
│ (базовая модель)│────────────>│    XGBoost       │
└─────────────────┘             └──────────────────┘
      ↓                                ↓
Предсказания Y₁                    Остатки ε
      ↓                                ↓
      └─────────┬──────────────────────┘
                ↓
        Финальный ансамбль:
        Y_final = Y₁ + α × ε
```

## Основные результаты

### Метрики качества
- **R²**: 0.8040 (+8.63% улучшение по сравнению с базовой моделью)
- **RMSE**: 0.8834 (-13.16% улучшение)
- **MAE**: 0.3533 (-11.78% улучшение)

### Важность признаков
1. **User_Count** (21.2%) - количество пользовательских оценок
2. **Platform** (9.9%) - игровая платформа
3. **Platform_User_Count** (7.7%) - средняя активность пользователей платформы
4. **Genre** (7.5%) - жанр игры
5. **Year_of_Release** (7.3%) - год выпуска

## Особенности реализации

- **Residual Modeling**: вместо простого усреднения моделей используется коррекция остатков
- **Оптимизация весов**: автоматический поиск оптимального коэффициента для residual модели
- **Direction Accuracy**: кастомная метрика для оценки правильности направления предсказания ошибки

## Проблемы и решения

- **Высокая корреляция остатков**: отказ от традиционного bagging в пользу residual modeling
- **Скошенность данных**: логарифмическое преобразование целевого признака

## Возможные улучшения

1. **Кастомные loss-функции** для asymmetric задач (концепция уже описана в **Insight**, на реализацию и тестирование не было времени)
5. **Кроссвалидация** 

## Автор

Проект выполнен в рамках изучения регрессионных ансамблей и гиперфикса на ML.
